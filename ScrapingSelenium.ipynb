{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a16c364e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "from selenium.webdriver.support.select import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c17b052b",
   "metadata": {},
   "outputs": [],
   "source": [
    "url =\"https://www.linkedin.com/jobs/search/?currentJobId=3724435212&geoId=102787409&keywords=data%20scientist&location=Maroc&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON&refresh=true\"             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961b3745",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6822c613",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deta import Deta\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "class DataManager:\n",
    "    def __init__(self, project_key, base_name):\n",
    "        self.deta = Deta(project_key)\n",
    "        self.db = self.deta.Base(base_name)\n",
    "\n",
    "    def save_to_database(self, data_frame, table_name):\n",
    "        # Charger les données existantes depuis la base de données\n",
    "        existing_jobs = [item['ID_job'] for item in self.db.fetch().items]\n",
    "\n",
    "        # Vérifier les doublons dans le DataFrame\n",
    "        data_frame = data_frame[~data_frame['ID_job'].isin(existing_jobs)]\n",
    "\n",
    "        # Itérer à travers chaque ligne du DataFrame\n",
    "        for index, row in data_frame.iterrows():\n",
    "#             # Convertir la date en format string avant l'insertion\n",
    "#             row['datePublish'] = row['datePublish'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "            date_du_jour = datetime.now().strftime(\"%d-%m-%Y\")\n",
    "            # Insérer la ligne dans la base de données\n",
    "            try:\n",
    "                self.db.put({'ID_job': row['ID_job'], 'companyName': row['companyName'],\n",
    "                             'jobTitles': row['jobTitles'], 'jobLinks': row['jobLinks'],\n",
    "                             'datePublish': row['datePublish'],'date_insertion':date_du_jour,'sent':0})\n",
    "            except Exception as e:\n",
    "                # Gérer les erreurs d'insertion\n",
    "                print(f\"Error inserting row {index + 1}: {str(e)}\")\n",
    "\n",
    "        print(\"Data inserted successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "493b7a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import schedule\n",
    "import time\n",
    "\n",
    "class LinkedinScrapping:\n",
    "    def __init__(self,date,field):\n",
    "        self.date=date\n",
    "        self.field=field\n",
    "        self.driver = None\n",
    "    def prepareEnv(self, url):\n",
    "        self.driver = webdriver.Chrome()\n",
    "        self.driver.get(url)\n",
    "    def getJobCount(self,url):\n",
    "        self.prepareEnv(url)  # Assurez-vous d'appeler la méthode pour initialiser le navigateur\n",
    "        wait = WebDriverWait(self.driver, 10)\n",
    "        job_count_element = wait.until(EC.presence_of_element_located((By.CLASS_NAME, \"results-context-header__job-count\")))\n",
    "\n",
    "        # Get the text from the job count element\n",
    "        job_count_text = job_count_element.text\n",
    "        numeric_value = re.sub(r'[^\\d]+', '', job_count_text)\n",
    "        y = pd.to_numeric(numeric_value)\n",
    "\n",
    "        return y\n",
    "    def exploreJobs(self,url):\n",
    "        driver = self.driver\n",
    "        max_failures = 3  # Set the maximum consecutive failures allowed\n",
    "        failures = 0\n",
    "\n",
    "        i = 0\n",
    "        while i <= 30:\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            i += 1\n",
    "\n",
    "            try:\n",
    "                # Find the 'See more jobs' button using XPath\n",
    "                x = driver.find_element(By.XPATH, \"//button[@aria-label='See more jobs']\")\n",
    "                driver.execute_script(\"arguments[0].click();\", x)\n",
    "                time.sleep(5)\n",
    "                failures = 0  # Reset the failure counter if successful\n",
    "                \n",
    "            except NoSuchElementException:\n",
    "                failures += 1\n",
    "                if failures >= max_failures:\n",
    "                    print(\"No more 'See more jobs' button found.\")\n",
    "                    break\n",
    "        self.driver=driver\n",
    "#___________________________________________________\n",
    "    def getElements(self, url):\n",
    "        \n",
    "            y = self.getJobCount(url)\n",
    "            self.exploreJobs(url)\n",
    "            driver=self.driver\n",
    "#             print(y)\n",
    "            companynames = []\n",
    "            job_titles = []\n",
    "            job_links = []\n",
    "            date_publish = []\n",
    "\n",
    "            # Find elements using By.CLASS_NAME\n",
    "            company_elements = driver.find_elements(By.CLASS_NAME, \"base-search-card__subtitle\")\n",
    "            job_elements = driver.find_elements(By.CLASS_NAME, \"base-search-card__title\")\n",
    "            link_elements = driver.find_elements(By.CLASS_NAME, \"base-card__full-link\")\n",
    "            date_elements = driver.find_elements(By.CLASS_NAME, \"job-search-card__listdate\")\n",
    "\n",
    "            for j in range(y):\n",
    "                # Check if there are enough elements in each list\n",
    "                if (\n",
    "                    j < len(company_elements)\n",
    "                    and j < len(job_elements)\n",
    "                    and j < len(link_elements)\n",
    "                    and j < len(date_elements)\n",
    "                ):\n",
    "                    # Extract the text from the elements\n",
    "                    company = company_elements[j].text\n",
    "                    job_title = job_elements[j].text\n",
    "                    link = link_elements[j].get_attribute(\"href\")\n",
    "                    date_publ = date_elements[j].text\n",
    "\n",
    "                    # Append the extracted data to the respective lists\n",
    "                    companynames.append(company)\n",
    "                    job_titles.append(job_title)\n",
    "                    job_links.append(link)\n",
    "                    date_publish.append(date_publ)\n",
    "                else:\n",
    "                    print(f\"Insufficient elements at index {j}. Skipping.\")\n",
    "\n",
    "            # Return a dictionary with the extracted data\n",
    "            return {\n",
    "                \"companyName\": companynames,\n",
    "                \"jobTitles\": job_titles,\n",
    "                \"jobLinks\": job_links,\n",
    "                \"datePublish\": date_publish\n",
    "            }\n",
    "    def convert_to_days(self, duration_string):\n",
    "        # Fonction pour convertir une chaîne de durée en jours\n",
    "        if \"week\" in duration_string:\n",
    "            weeks = int(re.search(r'\\d+', duration_string).group())\n",
    "            return weeks * 7\n",
    "        elif \"month\" in duration_string:\n",
    "            months = int(re.search(r'\\d+', duration_string).group())\n",
    "            return months * 30  # Assuming 1 month = 30 days (adjust as needed)\n",
    "        elif \"day\" in duration_string:\n",
    "            return int(re.search(r'\\d+', duration_string).group())\n",
    "        elif \"year\" in duration_string:\n",
    "            year = int(re.search(r'\\d+', duration_string).group())\n",
    "            return year * 365\n",
    "        else:\n",
    "            return 0\n",
    "    def getPandasData(self,data):\n",
    "            data=pd.DataFrame(data)\n",
    "            data[\"datePublish\"] = data[\"datePublish\"].apply(self.convert_to_days)\n",
    "            # Combinez \"datePublish\" et \"jobTitles\" pour former une clé primaire\n",
    "            data[\"ID_job\"] = data[\"datePublish\"].astype(str) + '_' + data[\"jobTitles\"]\n",
    "            print(data)\n",
    "            # Assurez-vous que la nouvelle colonne \"primary_key\" ne contient pas de doublons\n",
    "            if data[\"ID_job\"].duplicated().any():\n",
    "                print(\"Attention: Des doublons dans la clé primaire détectés!\")\n",
    "\n",
    "            return data\n",
    "    def save_to_database(self, data_frame):\n",
    "            project_key = \"a0zj6y7ndsx_ucmGqBMMLJu9PHPPSLtrpxKdg6Qj8J9Y\"  # Remplacez par votre clé de projet Deta\n",
    "            base_name = \"jobs\"      # Remplacez par le nom de votre base Deta\n",
    "\n",
    "            data_manager = DataManager(project_key, base_name)\n",
    "            data_manager.save_to_database(data_frame, table_name=\"jobs\")\n",
    "            print(\"\\n data saved \\n\")\n",
    "            \n",
    "    def scrape_and_save(self, url):\n",
    "            data = self.getElements(url)\n",
    "            pandas_data = self.getPandasData(data)\n",
    "            print(pandas_data)\n",
    "            self.save_to_database(pandas_data)\n",
    "\n",
    "    def schedule_scraping(self, url):\n",
    "            # Planifier le scraping toutes les heures\n",
    "#             schedule.every().hour.do(self.scrape_and_save, url, db_host, db_user, db_password, db_name, table_name)\n",
    "            schedule.every(10).minutes.do(self.scrape_and_save, url)\n",
    "            # Boucle pour exécuter la planification en continu\n",
    "            while True:\n",
    "                schedule.run_pending()\n",
    "                time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4217fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "linkedin_scraper = LinkedinScrapping(date=\"your_date\" , field=\"your_field\")\n",
    "linkedin_scraper.schedule_scraping(url=url)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af4aa34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a789cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040626a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb9fba0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e217208",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
